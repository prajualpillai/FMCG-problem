{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import nltk as nl\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Steps required for sentiment analysis\n",
    "## 1. Data Cleaning: Tokenization, stopword removal, stemming\n",
    "## 2. Vectorization: Text vectorization\n",
    "## 3. Text Classification: Based on content "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cleaning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df = fetch_20newsgroups()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "type(df)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rdf = df.data[:4]\n",
    "rdf"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stage 1: Convert into lower text"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "clean_df1 = []\n",
    "def to_lower(data):\n",
    "    for words in data:\n",
    "        clean_df1.append(str.lower(words))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "to_lower(rdf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stage 2: Tokenization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "ct2 = []\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "nl.download('punkt')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/prajualpillai/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sentance Tokenize: The sentances are broken up and stored as a 2d array"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sent_tok = []\n",
    "for sent in clean_df1:\n",
    "    sent = sent_tokenize(sent)\n",
    "    sent_tok.append(sent)\n",
    "sent_tok"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Word Tokenize: Each word in each sentance is broken up and is stored as a 2d array "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Word Tokenize\n",
    "ct2 = [word_tokenize(i) for i in clean_df1]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ct2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Removing punctuations and extra expressions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "import re\n",
    "cl3 = []\n",
    "for words in ct2:\n",
    "    c = []\n",
    "    for w in words:\n",
    "        res = re.sub(r'[^\\w\\s]',\"\",w)\n",
    "        if res != \"\":\n",
    "            c.append(res)\n",
    "    cl3.append(c)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cl3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Removing stopwords"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "nl.download('stopwords')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/prajualpillai/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "from nltk.corpus import stopwords"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "cl4 = []\n",
    "for words in cl3:\n",
    "    w = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            w.append(word)\n",
    "    cl4.append(w)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cl4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stemming"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "port = PorterStemmer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "a = [port.stem(i) for i in ['reading','wash','tilts']]\n",
    "a"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['read', 'wash', 'tilt']"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "cl5 = []\n",
    "for words in cl4:\n",
    "    w = []\n",
    "    for word in words:\n",
    "        w.append(port.stem(word))\n",
    "    cl5.append(w)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "cl5"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['lerxst',\n",
       "  'wamumdedu',\n",
       "  'thing',\n",
       "  'subject',\n",
       "  'car',\n",
       "  'nntppostinghost',\n",
       "  'rac3wamumdedu',\n",
       "  'organ',\n",
       "  'univers',\n",
       "  'maryland',\n",
       "  'colleg',\n",
       "  'park',\n",
       "  'line',\n",
       "  '15',\n",
       "  'wonder',\n",
       "  'anyon',\n",
       "  'could',\n",
       "  'enlighten',\n",
       "  'car',\n",
       "  'saw',\n",
       "  'day',\n",
       "  '2door',\n",
       "  'sport',\n",
       "  'car',\n",
       "  'look',\n",
       "  'late',\n",
       "  '60',\n",
       "  'earli',\n",
       "  '70',\n",
       "  'call',\n",
       "  'bricklin',\n",
       "  'door',\n",
       "  'realli',\n",
       "  'small',\n",
       "  'addit',\n",
       "  'front',\n",
       "  'bumper',\n",
       "  'separ',\n",
       "  'rest',\n",
       "  'bodi',\n",
       "  'know',\n",
       "  'anyon',\n",
       "  'tellm',\n",
       "  'model',\n",
       "  'name',\n",
       "  'engin',\n",
       "  'spec',\n",
       "  'year',\n",
       "  'product',\n",
       "  'car',\n",
       "  'made',\n",
       "  'histori',\n",
       "  'whatev',\n",
       "  'info',\n",
       "  'funki',\n",
       "  'look',\n",
       "  'car',\n",
       "  'pleas',\n",
       "  'email',\n",
       "  'thank',\n",
       "  'il',\n",
       "  'brought',\n",
       "  'neighborhood',\n",
       "  'lerxst'],\n",
       " ['guykuo',\n",
       "  'carsonuwashingtonedu',\n",
       "  'guy',\n",
       "  'kuo',\n",
       "  'subject',\n",
       "  'si',\n",
       "  'clock',\n",
       "  'poll',\n",
       "  'final',\n",
       "  'call',\n",
       "  'summari',\n",
       "  'final',\n",
       "  'call',\n",
       "  'si',\n",
       "  'clock',\n",
       "  'report',\n",
       "  'keyword',\n",
       "  'si',\n",
       "  'acceler',\n",
       "  'clock',\n",
       "  'upgrad',\n",
       "  'articleid',\n",
       "  'shelley1qvfo9innc3',\n",
       "  'organ',\n",
       "  'univers',\n",
       "  'washington',\n",
       "  'line',\n",
       "  '11',\n",
       "  'nntppostinghost',\n",
       "  'carsonuwashingtonedu',\n",
       "  'fair',\n",
       "  'number',\n",
       "  'brave',\n",
       "  'soul',\n",
       "  'upgrad',\n",
       "  'si',\n",
       "  'clock',\n",
       "  'oscil',\n",
       "  'share',\n",
       "  'experi',\n",
       "  'poll',\n",
       "  'pleas',\n",
       "  'send',\n",
       "  'brief',\n",
       "  'messag',\n",
       "  'detail',\n",
       "  'experi',\n",
       "  'procedur',\n",
       "  'top',\n",
       "  'speed',\n",
       "  'attain',\n",
       "  'cpu',\n",
       "  'rate',\n",
       "  'speed',\n",
       "  'add',\n",
       "  'card',\n",
       "  'adapt',\n",
       "  'heat',\n",
       "  'sink',\n",
       "  'hour',\n",
       "  'usag',\n",
       "  'per',\n",
       "  'day',\n",
       "  'floppi',\n",
       "  'disk',\n",
       "  'function',\n",
       "  '800',\n",
       "  '14',\n",
       "  'floppi',\n",
       "  'especi',\n",
       "  'request',\n",
       "  'summar',\n",
       "  'next',\n",
       "  'two',\n",
       "  'day',\n",
       "  'pleas',\n",
       "  'add',\n",
       "  'network',\n",
       "  'knowledg',\n",
       "  'base',\n",
       "  'done',\n",
       "  'clock',\n",
       "  'upgrad',\n",
       "  'nt',\n",
       "  'answer',\n",
       "  'poll',\n",
       "  'thank',\n",
       "  'guy',\n",
       "  'kuo',\n",
       "  'guykuo',\n",
       "  'uwashingtonedu'],\n",
       " ['twilli',\n",
       "  'ececnpurdueedu',\n",
       "  'thoma',\n",
       "  'e',\n",
       "  'willi',\n",
       "  'subject',\n",
       "  'pb',\n",
       "  'question',\n",
       "  'organ',\n",
       "  'purdu',\n",
       "  'univers',\n",
       "  'engin',\n",
       "  'comput',\n",
       "  'network',\n",
       "  'distribut',\n",
       "  'usa',\n",
       "  'line',\n",
       "  '36',\n",
       "  'well',\n",
       "  'folk',\n",
       "  'mac',\n",
       "  'plu',\n",
       "  'final',\n",
       "  'gave',\n",
       "  'ghost',\n",
       "  'weekend',\n",
       "  'start',\n",
       "  'life',\n",
       "  '512k',\n",
       "  'way',\n",
       "  'back',\n",
       "  '1985',\n",
       "  'sooo',\n",
       "  'market',\n",
       "  'new',\n",
       "  'machin',\n",
       "  'bit',\n",
       "  'sooner',\n",
       "  'intend',\n",
       "  'look',\n",
       "  'pick',\n",
       "  'powerbook',\n",
       "  '160',\n",
       "  'mayb',\n",
       "  '180',\n",
       "  'bunch',\n",
       "  'question',\n",
       "  'hope',\n",
       "  'somebodi',\n",
       "  'answer',\n",
       "  'anybodi',\n",
       "  'know',\n",
       "  'dirt',\n",
       "  'next',\n",
       "  'round',\n",
       "  'powerbook',\n",
       "  'introduct',\n",
       "  'expect',\n",
       "  'heard',\n",
       "  '185c',\n",
       "  'suppos',\n",
       "  'make',\n",
       "  'appear',\n",
       "  'summer',\n",
       "  'nt',\n",
       "  'heard',\n",
       "  'anymor',\n",
       "  'sinc',\n",
       "  'nt',\n",
       "  'access',\n",
       "  'macleak',\n",
       "  'wonder',\n",
       "  'anybodi',\n",
       "  'info',\n",
       "  'anybodi',\n",
       "  'heard',\n",
       "  'rumor',\n",
       "  'price',\n",
       "  'drop',\n",
       "  'powerbook',\n",
       "  'line',\n",
       "  'like',\n",
       "  'one',\n",
       "  'duo',\n",
       "  'went',\n",
       "  'recent',\n",
       "  'impress',\n",
       "  'display',\n",
       "  '180',\n",
       "  'could',\n",
       "  'probabl',\n",
       "  'swing',\n",
       "  '180',\n",
       "  'got',\n",
       "  '80mb',\n",
       "  'disk',\n",
       "  'rather',\n",
       "  '120',\n",
       "  'nt',\n",
       "  'realli',\n",
       "  'feel',\n",
       "  'much',\n",
       "  'better',\n",
       "  'display',\n",
       "  'yea',\n",
       "  'look',\n",
       "  'great',\n",
       "  'store',\n",
       "  'wow',\n",
       "  'realli',\n",
       "  'good',\n",
       "  'could',\n",
       "  'solicit',\n",
       "  'opinion',\n",
       "  'peopl',\n",
       "  'use',\n",
       "  '160',\n",
       "  '180',\n",
       "  'daytoday',\n",
       "  'worth',\n",
       "  'take',\n",
       "  'disk',\n",
       "  'size',\n",
       "  'money',\n",
       "  'hit',\n",
       "  'get',\n",
       "  'activ',\n",
       "  'display',\n",
       "  'realiz',\n",
       "  'real',\n",
       "  'subject',\n",
       "  'question',\n",
       "  'play',\n",
       "  'around',\n",
       "  'machin',\n",
       "  'comput',\n",
       "  'store',\n",
       "  'breifli',\n",
       "  'figur',\n",
       "  'opinion',\n",
       "  'somebodi',\n",
       "  'actual',\n",
       "  'use',\n",
       "  'machin',\n",
       "  'daili',\n",
       "  'might',\n",
       "  'prove',\n",
       "  'help',\n",
       "  'well',\n",
       "  'hellcat',\n",
       "  'perform',\n",
       "  'thank',\n",
       "  'bunch',\n",
       "  'advanc',\n",
       "  'info',\n",
       "  'could',\n",
       "  'email',\n",
       "  'post',\n",
       "  'summari',\n",
       "  'news',\n",
       "  'read',\n",
       "  'time',\n",
       "  'premium',\n",
       "  'final',\n",
       "  'around',\n",
       "  'corner',\n",
       "  'tom',\n",
       "  'willi',\n",
       "  'twilli',\n",
       "  'ecnpurdueedu',\n",
       "  'purdu',\n",
       "  'electr',\n",
       "  'engin',\n",
       "  'convict',\n",
       "  'danger',\n",
       "  'enemi',\n",
       "  'truth',\n",
       "  'lie',\n",
       "  'f',\n",
       "  'w',\n",
       "  'nietzsch'],\n",
       " ['jgreen',\n",
       "  'amber',\n",
       "  'joe',\n",
       "  'green',\n",
       "  'subject',\n",
       "  'weitek',\n",
       "  'p9000',\n",
       "  'organ',\n",
       "  'harri',\n",
       "  'comput',\n",
       "  'system',\n",
       "  'divis',\n",
       "  'line',\n",
       "  '14',\n",
       "  'distribut',\n",
       "  'world',\n",
       "  'nntppostinghost',\n",
       "  'amberssdcsdharriscom',\n",
       "  'xnewsread',\n",
       "  'tin',\n",
       "  'version',\n",
       "  '11',\n",
       "  'pl9',\n",
       "  'robert',\n",
       "  'jc',\n",
       "  'kyanko',\n",
       "  'rob',\n",
       "  'rjckuucp',\n",
       "  'wrote',\n",
       "  'abraxi',\n",
       "  'iastateedu',\n",
       "  'write',\n",
       "  'articl',\n",
       "  'abraxis734340159',\n",
       "  'class1iastateedu',\n",
       "  'anyon',\n",
       "  'know',\n",
       "  'weitek',\n",
       "  'p9000',\n",
       "  'graphic',\n",
       "  'chip',\n",
       "  'far',\n",
       "  'lowlevel',\n",
       "  'stuff',\n",
       "  'goe',\n",
       "  'look',\n",
       "  'pretti',\n",
       "  'nice',\n",
       "  'got',\n",
       "  'quadrilater',\n",
       "  'fill',\n",
       "  'command',\n",
       "  'requir',\n",
       "  'four',\n",
       "  'point',\n",
       "  'weitek',\n",
       "  'addressphon',\n",
       "  'number',\n",
       "  'like',\n",
       "  'get',\n",
       "  'inform',\n",
       "  'chip',\n",
       "  'joe',\n",
       "  'green',\n",
       "  'harri',\n",
       "  'corpor',\n",
       "  'jgreen',\n",
       "  'csdharriscom',\n",
       "  'comput',\n",
       "  'system',\n",
       "  'divis',\n",
       "  'thing',\n",
       "  'realli',\n",
       "  'scare',\n",
       "  'person',\n",
       "  'sens',\n",
       "  'humor',\n",
       "  'jonathan',\n",
       "  'winter']]"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}